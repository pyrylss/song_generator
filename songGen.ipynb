{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "b63caccd",
      "metadata": {
        "id": "b63caccd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "ef7b8c82",
      "metadata": {
        "id": "ef7b8c82"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "ac21b155",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac21b155",
        "outputId": "24b6a928-95aa-4bc7-c480-a50f5057c450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "file_name = 'lyrics.zip'\n",
        "dir_path = ''\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "    zip.extractall(path=dir_path)\n",
        "    print('Done')\n",
        "\n",
        "text = ''\n",
        "\n",
        "# os.listdir(dir_path) returns a list of filenames in the directory\n",
        "for filename in os.listdir('./lyrics'):\n",
        "    # Check if the file is a .txt file\n",
        "    if filename.endswith('.txt'):\n",
        "        with open(os.path.join('./lyrics', filename), 'r', encoding='utf-8') as f:\n",
        "            text += f.read().lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "pOtwd0YWOcEW",
      "metadata": {
        "id": "pOtwd0YWOcEW"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "n_features = 348\n",
        "n_heads = 6\n",
        "n_layers = 6\n",
        "dropout = 0.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "learning_rate = 3e-4\n",
        "eval_interval = 500\n",
        "max_iters = 18000\n",
        "eval_iters = 200\n",
        "temperature = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "f78be6a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f78be6a6",
        "outputId": "457d71af-34f3-478e-fbcf-c19fa363ff59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6731387\n"
          ]
        }
      ],
      "source": [
        "print(len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "BzIdlUMYBhsV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzIdlUMYBhsV",
        "outputId": "4be48631-6d2e-4b62-e0a6-9119318f89dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "126"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "0160a6b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0160a6b3",
        "outputId": "f1caa759-ecf6-48d5-de51-fe1f5f229706",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', '@', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '¡', '¢', '¨', '©', 'ª', '«', '¬', '\\xad', '±', '²', '³', '´', '¶', '»', '¼', '¿', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'ð', 'ñ', 'ò']\n",
            "\t\n",
            " !\"#$%&'()*+,-./0123456789:;=>?@[]_`abcdefghijklmnopqrstuvwxyz{|}¡¢¨©ª«¬­±²³´¶»¼¿ßàáâãäåçèéêëìíðñòóôõöùúüþƒ–—‘’‚“”†•…€∆∑√≠≥﻿\n"
          ]
        }
      ],
      "source": [
        "print(chars[:100])\n",
        "print(''.join(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "17a0185b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17a0185b",
        "outputId": "5b525f78-5e31-4cab-d991-8ee5b65f5a64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[45, 42, 49, 49, 52, 2, 60, 52, 55, 49, 41]\n",
            "hello world\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "print(encode(\"hello world\"))\n",
        "print(decode(encode(\"hello world\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "c071aff5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c071aff5",
        "outputId": "d965a1a3-4188-4114-970e-7c8eefcf4221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([6731387]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "ea67471d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea67471d",
        "outputId": "679ec123-839b-4201-9b2b-1a19a24e29cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6058248 673139\n"
          ]
        }
      ],
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(len(train_data), len(val_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "2ac5636e",
      "metadata": {
        "id": "2ac5636e"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split=='train' else val_data\n",
        "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "AWZLejYrOVrh",
      "metadata": {
        "id": "AWZLejYrOVrh"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "ICjMOddStN2H",
      "metadata": {
        "id": "ICjMOddStN2H"
      },
      "outputs": [],
      "source": [
        "def subsequent_mask(sz):\n",
        "    mask = (torch.tril(torch.ones(sz, sz)))\n",
        "    mask = mask.masked_fill(mask[:sz, :sz] == 0, float('-inf'))\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "HzGMsi5FtO1Z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "HzGMsi5FtO1Z",
        "outputId": "b10dc1d6-694f-4abc-a41f-ba70bfff8673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [1., 1., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [1., 1., 1., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [1., 1., 1., 1., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [1., 1., 1., 1., 1., -inf, -inf, -inf, -inf, -inf],\n",
            "        [1., 1., 1., 1., 1., 1., -inf, -inf, -inf, -inf],\n",
            "        [1., 1., 1., 1., 1., 1., 1., -inf, -inf, -inf],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., -inf, -inf],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., -inf],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7bd50c931cf0>"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "mask = subsequent_mask(10)\n",
        "print(mask)\n",
        "plt.imshow(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "bd0c832a",
      "metadata": {
        "id": "bd0c832a"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_features, n_heads):\n",
        "        super().__init__()\n",
        "        self.mha = nn.MultiheadAttention(n_features, n_heads, dropout=dropout)\n",
        "        self.ln1 = nn.LayerNorm(n_features)\n",
        "        self.drop1 = nn.Dropout(dropout)\n",
        "        self.drop2 = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(n_features, 4 * n_features),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_features, n_features),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(n_features)\n",
        "\n",
        "        #self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.to(device)\n",
        "\n",
        "        attn_in = x.permute(1, 0, 2) # (B, T, C) --> (T, B, C)\n",
        "        attn_output, _ = self.mha(self.ln1(attn_in), self.ln1(attn_in), self.ln1(attn_in), attn_mask=mask)  # attn_output has the same shape as attn_in\n",
        "        attn_output = attn_output.permute(1, 0, 2) # (T, B, C) --> (B, T, C)\n",
        "\n",
        "        x = self.drop1(x) + attn_output\n",
        "        x = self.drop2(x) + self.ff(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "652a5ecc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "652a5ecc",
        "outputId": "ccf455d8-cc89-465e-86aa-b2e761d1dc20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t≠#iìõ\n",
            "u+﻿¿u€ì*!¨±«4/=≠nú:y…1€m”0.ùpääv?õx´∑≠6≠][kiâõ≥f´kgõ¬5âç:ç|ô;ôè2b∑åâégi¡õz-à√o]?íê\"•ö¶.ö '?&€j\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "class SongGenerator(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.embedding = nn.Embedding(vocab_size, n_features)\n",
        "\n",
        "        self.pos_encoding = nn.Embedding(block_size, n_features)\n",
        "        self.blocks = nn.Sequential(*[Block(n_features, n_heads) for _ in range(n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(n_features) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_features, vocab_size)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape\n",
        "        mask = subsequent_mask(T).to(device)\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.dropout1(self.embedding(idx)) # (B,T,C)\n",
        "        pos_emb = self.pos_encoding(torch.arange(T, device=device)) # (T, C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        #x = self.blocks(x, mask=mask) # (B, T, C)\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask=mask)\n",
        "\n",
        "        x = self.dropout2(self.ln_f(x)) # (B, T, C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=temperature):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            #logits = logits/temperature\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next.T), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = SongGenerator()\n",
        "model = model.to(device)\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "Xu5N_4tkTzCl",
      "metadata": {
        "id": "Xu5N_4tkTzCl"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, verbose=True)\n",
        "#scheduler ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "0e602e70",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0e602e70",
        "outputId": "c44785d0-d078-4e39-bebd-f5cc5fbaf715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 1.4101, val loss 1.4297\n",
            "\n",
            "cause having its and love when the trees a proper\n",
            "merovondra hardy\n",
            "be me its were wanna fice is pera\n",
            "step 500: train loss 1.3952, val loss 1.4119\n",
            "\n",
            " think very gamights. lonelights do jurity. light cares and more the broke may, talks  he king\n",
            "for t\n",
            "step 1000: train loss 1.3813, val loss 1.3995\n",
            "\n",
            "\n",
            "i stoness is dreami tonging her head-from me ahw conganssionsence\n",
            "one miss the for\n",
            "shiny sky isn't \n",
            "step 1500: train loss 1.3756, val loss 1.3920\n",
            "\n",
            "white name to yarders\n",
            "that they baby\n",
            "    fire will here. \n",
            " firever woments\n",
            "eenily, hand bottle achnr\n",
            "step 2000: train loss 1.3704, val loss 1.3864\n",
            "\n",
            "cause i hop\n",
            "i got a little drew\n",
            "cristed saen)\n",
            "it's goin' seen a never fin with at there day\n",
            "dgtch ca\n",
            "step 2500: train loss 1.3642, val loss 1.3823\n",
            "\n",
            "[outh itâ. cheprefrain but heeothelht jughed\n",
            "jorghee wanna goin'\n",
            "hey chie, my do-finna stoat so but \n",
            "step 3000: train loss 1.3570, val loss 1.3739\n",
            "\n",
            "here, heare, your pempty's matic\n",
            "heart, you're deciny mondom\n",
            "maims\n",
            "so you just malp almail for you h\n",
            "step 3500: train loss 1.3548, val loss 1.3732\n",
            "\n",
            "boy it, more, pict, pull at fall u-boy\n",
            "it all the michaelp cit, reach and fills girl\n",
            "i'll because dr\n",
            "step 4000: train loss 1.3504, val loss 1.3725\n",
            "\n",
            "u're 2 a had the holins cream black ows\n",
            "don't hung me to pray shit\n",
            "huy-palet\n",
            "but i’d say the we say \n",
            "step 4500: train loss 1.3435, val loss 1.3647\n",
            "\n",
            "elike you hellolt patyen\n",
            "now\n",
            "geaving. you know up no geas listenet\n",
            "when yeahphus, spif show you new \n",
            "step 5000: train loss 1.3483, val loss 1.3642\n",
            "\n",
            "they wanna palino weat rantly night and undo as wears\n",
            "and mmmmmmms i dont wanna mass some boless and\n",
            "step 5500: train loss 1.3327, val loss 1.3542\n",
            "\n",
            "heart, favorite, greelhigh! shootice, sholy anyway, yeah\n",
            "every time ponery time more beauties\n",
            "that's\n",
            "step 6000: train loss 1.3214, val loss 1.3429\n",
            "\n",
            "that left lost wanted in that want i wants edors\n",
            "who this love your enme\n",
            "will the nably tement say i\n",
            "step 6500: train loss 1.3208, val loss 1.3426\n",
            "\n",
            "pointailed a me top\n",
            "that's begging taff\n",
            "this ts come\n",
            "before tellied to hold \n",
            "botherly but lies chain\n",
            "step 7000: train loss 1.3166, val loss 1.3427\n",
            "\n",
            "my be patoor rolling in love with the icedes\n",
            "i get a nig bare bunk\n",
            "andining baddadda what jack up\n",
            "sh\n",
            "step 7500: train loss 1.3007, val loss 1.3233\n",
            "\n",
            "while\n",
            "i out 99 noon and thought that alone lot\n",
            "it don't make the pains ramport any range, thed alone\n",
            "step 8000: train loss 1.3109, val loss 1.3262\n",
            "\n",
            "no very vizie\n",
            "vims\n",
            "ye\n",
            "yes, letsant loist through the bird\n",
            "ye\n",
            "they'll be things fornings hit doing ac\n",
            "step 8500: train loss 1.3067, val loss 1.3308\n",
            "\n",
            "i i seproples so a too see glow your slow we roll like a broke\n",
            "most what you can't played hyve\n",
            "i alw\n",
            "step 9000: train loss 1.2900, val loss 1.3131\n",
            "\n",
            "baby, oh, running\n",
            "i don’t look at yours, baby whnny have you look at alone mise\n",
            "i febrain, let me pe\n",
            "step 9500: train loss 1.2855, val loss 1.3146\n",
            "\n",
            "mayonk\n",
            "came fall can sorry my your asleep\n",
            "tell my listen is my mrzican\n",
            "walks ress tonight\n",
            "tell cut o\n",
            "step 10000: train loss 1.2919, val loss 1.3146\n",
            "\n",
            "dadonds yeah yeah yeah yeah yeah yeah yes aheyes yeah yeah\n",
            "dadons yeah yeah we can be mine\n",
            "so your f\n",
            "step 10500: train loss 1.2869, val loss 1.3064\n",
            "\n",
            "menviiizey went our baby this\n",
            "smokin' or your fair fair\n",
            "thing with us hands on the city, fo thing th\n",
            "step 11000: train loss 1.2924, val loss 1.3147\n",
            "\n",
            "? can't i had your heart (it)\n",
            "but even if you tryna seebly at all tta suttie, oh, sutta, i'm trynaw\n",
            "\n",
            "step 11500: train loss 1.2806, val loss 1.3044\n",
            "\n",
            "cah this is love he stray to go? i try to love to love her get on my mommi am,\n",
            "just would be ready o\n",
            "step 12000: train loss 1.2784, val loss 1.2974\n",
            "\n",
            "ny on you pretny\n",
            "you'fraid you can't stop the fashing\n",
            "it ain't ready, you know that you gonna guess\n",
            "\n",
            "step 12500: train loss 1.2604, val loss 1.2837\n",
            "\n",
            "i can cocked note\n",
            "and you there's notel\n",
            "doctop me nothing to show the hookn. thats my now that\n",
            "in th\n",
            "step 13000: train loss 1.2752, val loss 1.2901\n",
            "\n",
            "love is all\n",
            "youll of deferners\n",
            "i stood, maybe you know, you writin' love me\n",
            "you was aural one\n",
            "someti\n",
            "step 13500: train loss 1.2534, val loss 1.2754\n",
            "\n",
            "me knoca calls pass swin the stang what i'm feeling sun\n",
            "but colds\n",
            "let even still the point besiding \n",
            "step 14000: train loss 1.2615, val loss 1.2846\n",
            "\n",
            "from teccuss dislys***) \n",
            "so i want my troubt, and the way you ask a look shooker.\n",
            "my strippers and c\n",
            "step 14500: train loss 1.2600, val loss 1.2873\n",
            "\n",
            "       ;               ,             |\n",
            "        |i         f,        |              |             |  \n",
            "step 15000: train loss 1.2547, val loss 1.2907\n",
            "\n",
            "strong\n",
            "\n",
            "give my stair\n",
            "gastangehing gang gang but gange to me\n",
            "\n",
            "for langes and to me\n",
            "love me\n",
            "trylandon\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-6142fc62b55d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf')  # start with a high value\n",
        "no_improve_counter = 0\n",
        "patience = 3\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        #scheduler.step(losses['val'])\n",
        "        context = torch.tensor([[stoi['\\n']]], dtype=torch.long, device=device)\n",
        "        print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))\n",
        "\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            no_improve_counter = 0\n",
        "        else:\n",
        "            no_improve_counter += 1\n",
        "            if no_improve_counter > patience:\n",
        "                print(\"Early stopping!\")\n",
        "                break\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "vywib7TFcyuD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vywib7TFcyuD",
        "outputId": "5294e998-9d55-4a36-9e48-732187072735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "cause im lady slip on life\n",
            "i a relivingle out like a yelionhase\n",
            "its befollow the gone and you sad\n",
            "there gon remember thangs tells you\n",
            "i want this tripped of the blackforlorve a boyf\n",
            "and buside in missine secons dick\n",
            "that soliarints\n",
            "my is scauy cags brinds panations\n",
            "flets my rocks grappers i say that baby\n",
            "remily in loolin mojaip\n",
            "we gonna duda climb mornin\n",
            "come on us ano\n",
            "shit get an all\n",
            "no exist getting up dungs\n",
            "to loor no i put itation to side truth\n",
            "ilsuch a sacross fill the hall\n",
            "every day\n",
            "into pling new kergest kmarl no thoses hills puth\n",
            "said why we wont grew goreens the thing lands wono shit\n",
            "i go the wall for belms you tour has are about so cadie free\n",
            "see you backed runner the aitesses\n",
            "could shit\n",
            "my reason up today and sto feath\n",
            "i baby you ball tacks he wasle\n",
            "theses if them suse we can come of asfit\n",
            "ballela\n",
            "sle in a beat grab hops to all ask\n",
            "its shit the cave like shit feelm coloznin candles go what i all\n",
            "shope play your shit all rises and green day\n",
            "clip shaawed baby\n",
            "misely donmonine \n"
          ]
        }
      ],
      "source": [
        "# after 25 000 epochs, train loss: 1.25, val loss: 1.29\n",
        "\n",
        "# generate from the model\n",
        "context = torch.tensor([[stoi['\\n']]], dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=1000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "Zb7bOHFXluHC",
      "metadata": {
        "id": "Zb7bOHFXluHC"
      },
      "outputs": [],
      "source": [
        "# save the model before finetuning\n",
        "torch.save(model.state_dict(), 'songGen.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "fl8QmnK9HyGF",
      "metadata": {
        "id": "fl8QmnK9HyGF"
      },
      "outputs": [],
      "source": [
        "output = decode(model.generate(context, max_new_tokens=2000)[0].tolist())\n",
        "with open('model_output.txt', 'w') as f:\n",
        "  f.write(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "O22C2Iox9vYI",
      "metadata": {
        "id": "O22C2Iox9vYI"
      },
      "outputs": [],
      "source": [
        "file_name = 'eminem.txt'\n",
        "\n",
        "with open('./eminem.txt', 'r') as f:\n",
        "  text_e = f.read().lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "a-21tEzQByXk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-21tEzQByXk",
        "outputId": "aa553056-916a-494b-dead-54c5f1fc1baf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([323124]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "data_e = torch.tensor(encode(text_e), dtype=torch.long)\n",
        "print(data_e.shape, data_e.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "yMY8_CukCHW0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMY8_CukCHW0",
        "outputId": "fb68c3b1-d3bc-4f16-b192-92567a24ac79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "290811 32313\n"
          ]
        }
      ],
      "source": [
        "n = int(0.9*len(data_e))\n",
        "train_data = data_e[:n]\n",
        "val_data = data_e[n:]\n",
        "print(len(train_data), len(val_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "Z4dIu5XxGIHg",
      "metadata": {
        "id": "Z4dIu5XxGIHg"
      },
      "outputs": [],
      "source": [
        "new_learning_rate = 1e-5\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=new_learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "QQ7PgNDJG1w3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "QQ7PgNDJG1w3",
        "outputId": "3650a9ea-2670-4c56-b59e-4d4bdc72c347"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 1.3061, val loss 1.4314\n",
            "\n",
            "beleyon no stormm, who woodn't some nans\n",
            "straight lyin' clue, a mome and shot you raution\n",
            "introdutes\n",
            "step 99: train loss 1.2964, val loss 1.4294\n",
            "\n",
            "create for manmartnes lit of mamama\n",
            "i want to come man can i want to asleeportant my for the gosta\n",
            "'\n",
            "step 198: train loss 1.2970, val loss 1.4284\n",
            "\n",
            "working at the family\n",
            "he found everybody back on my first dreams to get mom town back\n",
            "and the pack d\n",
            "step 297: train loss 1.2896, val loss 1.4342\n",
            "\n",
            "  the bolt doard \n",
            "   life to this eat town or mount ircalt em\n",
            "  imnanairer, prodump-hash to posaun\n",
            "w\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-32117b7bb05b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for iter in range(1000):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % 99 == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        #scheduler.step(losses['val'])\n",
        "        context = torch.tensor([[stoi['\\n']]], dtype=torch.long, device=device)\n",
        "        print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "PTCksRfUI-0i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTCksRfUI-0i",
        "outputId": "5cf44896-4d40-4343-d7ab-3744c201c0cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "get you go\n",
            "play in apart\n",
            "it's play in intestand peepeepeep, or baby c.\n",
            "but blaze on this more nowore man\n",
            "some will east why you might do\n",
            "wear that tree been eveberses, it's in fast here,\n",
            "your might blave nice\n",
            "or hange my sugar — let's go play alar ate\n",
            "i'm one motherfuckin' days so thing to pail big, fuckin' butt it what i'll be trough to bad\n",
            "your shit lit we look for the pail\n",
            "look gonna be the bad\n",
            "underfuckin' liart with s night on i had\n",
            "become just did my enemy did i'm gonna back i had shit me, ah i even stuck hits \n",
            "yyiach god, known, that mornin', even cuffin'\n",
            "but i make the words i make it volatigned these gimal\n",
            "words to had that some him, dadde i float at stars why didn't mabiep it aiuse, it didn't play but yet like i ladie even, promecess\n",
            "i feel, i got drunkin' i'm (and to broya\n",
            "my must be right up in one other)\n",
            "cause you my little other man and hope is rit so way\n",
            "appeal with a waxeron\n",
            "and i leave you pull me a calling?\n",
            "its beawing\n",
            "i got a lot of beef and better zreat of of the pl\n"
          ]
        }
      ],
      "source": [
        "# after 1000 iters\n",
        "context = torch.tensor([[stoi['\\n']]], dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=1000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "Ni6TVon3HCwO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni6TVon3HCwO",
        "outputId": "2aae213f-53f4-4a30-96e6-c3bd5839ad2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "s baby, oh the momma go down\n",
            "and shawe hear it down charing\n",
            "in drama-ledging monster, you're only hear baning so head\n",
            "oh love her\n",
            "oh my world hear it os runnin' cldinot soul\n",
            "so you're feeling messary one so picture shally done but i feeling mess game\n",
            "when you has harden, put the churred his clubder for my ahocket\n",
            "you and you pray ownch\n",
            "i'll red five you but to this you\n",
            "you think i're table you no longeratiuse\n",
            "brignes beat you knocked looker, i'm in the reslelared\n",
            "to tell the coupe my raised mom just like when kissed you make you become \n",
            "in the the grupt\n",
            "my repleminidior, his hood you want to pussy on the through\n",
            "while me, i was missiwing the the sent cance assigns off inside the cast\n",
            "absave me people to be the feather's somedhiors why i'll scape, he thard ever go kelly i drop him, hongs\n",
            "getting's to shi i racking fortuner;\n",
            "\n",
            "must proof missy god, hole, send me whit they only feel her stilcape\n",
            "let's talk, mistake my bo hold\n",
            "i send, what abow maken he my bock?\n",
            "and she's rightinoc parcapin\n"
          ]
        }
      ],
      "source": [
        "# after 1300 iters\n",
        "context = torch.tensor([[stoi['\\n']]], dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=1000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "mzQhcZ0XCwv8",
      "metadata": {
        "id": "mzQhcZ0XCwv8"
      },
      "outputs": [],
      "source": [
        "output_e = decode(model.generate(context, max_new_tokens=2500)[0].tolist())\n",
        "with open('model_output_eminem.txt', 'w') as f:\n",
        "  f.write(output_e)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
